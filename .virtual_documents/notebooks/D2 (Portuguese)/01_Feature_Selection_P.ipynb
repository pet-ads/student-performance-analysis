import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

d2 = pd.read_csv("d2_trabalhado.csv", sep=",")


X = d2.drop(['G3'], axis=1)
y = d2['G3']





from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

f_regression = SelectKBest(score_func=f_regression, k=8)
# k = 4 (é o número de variáveis selecionadas)


fit = f_regression.fit(X, y)
features = fit.transform(X)


print(features)


cols = fit.get_support(indices=True)
d2.iloc[:, cols]





from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE

model = LogisticRegression(solver='saga', max_iter=5000)
rfe = RFE(model, n_features_to_select=6, step=2)


fit = rfe.fit(X,y)


print("Número de features: {}".format(fit.n_features_))


cols = fit.get_support(indices=True)
d2.iloc[:, cols]





from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=10)
model.fit(X,y)

print(model.feature_importances_)


feature_importances = pd.DataFrame(model.feature_importances_,
                                   index = X.columns,
                                   columns = ['importance']).sort_values('importance',
                                   ascending=False)
feature_importances





from sklearn.tree import DecisionTreeRegressor
clf = DecisionTreeRegressor(random_state=0)


clf = clf.fit(X,y)


clf.feature_importances_


feature_names = X.columns
importances = clf.feature_importances_

feature_importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

feature_importances = feature_importances.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(16, 8))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.gca().invert_yaxis()
plt.show()






