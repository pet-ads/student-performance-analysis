import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

d2 = pd.read_csv("d2_trabalhado.csv", sep=",")


X = d2.drop(['G3'], axis=1)
y = d2['G3']


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


regressor = LinearRegression()
regressor.fit(X_train, y_train)


y_pred = regressor.predict(X_test)





mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')





selected_features = ['G2','famrel','studytime', 'health', 'Dalc']


X = d2[selected_features]
y = d2['G3']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

regressor = LinearRegression()
regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')





from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

X = d2[selected_features]
y = d2['G3']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.transform(X_test)

model = LinearRegression()
model.fit(X_poly_train, y_train)

y_pred = model.predict(X_poly_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error: %.2f" % mse)
print("R^2 Score: %.2f" % r2)





from sklearn.linear_model import Lasso

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lasso = Lasso(alpha=0.1)

lasso.fit(X_train, y_train)
y_pred = lasso.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error: %.2f" % mse)
print("R^2 Score: %.2f" % r2)

print("Coeficientes:", lasso.coef_)





from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

selected_features = ['G2', 'famrel', 'schoolsup','failures']
#selected_features = ['G1', 'G2', 'schoolsup']

X = d2[selected_features]
y = d2['G3']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

decision_tree = DecisionTreeRegressor(random_state=42)

decision_tree.fit(X_train, y_train)
y_pred = decision_tree.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error: %.2f" % mse)
print("R^2 Score: %.2f" % r2)





from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
import numpy as np

feature_sets = [
    ['G2', 'famrel', 'schoolsup', 'failures'],
    ['famrel', 'G2', 'schoolsup', 'age'],
    ['G2', 'famrel', 'absences', 'Dalc'],
    ['G2', 'famrel', 'absences', 'romantic'],
     ['G2', 'famrel', 'absences', 'failures'],
     ['G2', 'schoolsup', 'freetime', 'famrel'],
    ['G2', 'Medu', 'studytime', 'failures']
]

for features in feature_sets:
    X = d2[features]
    y = d2['G3']

    decision_tree = DecisionTreeRegressor(random_state=42)
    
    scores = cross_val_score(decision_tree, X, y, cv=5, scoring='neg_mean_squared_error')
    
    mse_scores = -scores
    mean_mse = np.mean(mse_scores)
    std_mse = np.std(mse_scores)
    
    print(f"Features: {features}")
    print(f"Mean Squared Error (Cross-Validation): {mean_mse:.2f} Â± {std_mse:.2f}")
    print()

