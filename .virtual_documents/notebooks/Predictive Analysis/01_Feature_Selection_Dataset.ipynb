import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

d1 = pd.read_csv("d1_trabalhado.csv", sep=",")


X = d1.drop(['G3'], axis=1)
y = d1['G3']





from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

f_regression = SelectKBest(score_func=f_regression, k=6)
# k = 4 (é o número de variáveis selecionadas)


fit = f_regression.fit(X, y)


features = fit.transform(X)


print(features)


cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE


model = LogisticRegression(solver='saga', max_iter=5000)
rfe = RFE(model, n_features_to_select=6, step=2)


fit = rfe.fit(X,y)


print("Número de features: {}".format(fit.n_features_))


cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.ensemble import RandomForestRegressor


model = RandomForestRegressor(n_estimators=10)
model.fit(X,y)


print(model.feature_importances_)


feature_importances = pd.DataFrame(model.feature_importances_,
                                   index = X.columns,
                                   columns = ['importance']).sort_values('importance',
                                   ascending=False)


feature_importances


feature_importances.plot(kind='bar')





from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.15)
selector.fit(X)

features_selecionadas = X.columns[selector.get_support()]
features_removidas = X.columns[~selector.get_support()]

print("Features selecionadas:", features_selecionadas)
print("Features removidas:", features_removidas)





from sklearn.tree import DecisionTreeRegressor
clf = DecisionTreeRegressor(random_state=0)


clf = clf.fit(X,y)


clf.feature_importances_


feature_names = X.columns
importances = clf.feature_importances_

feature_importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

feature_importances = feature_importances.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.gca().invert_yaxis()
plt.show()





from sklearn.feature_selection import SelectFromModel


estimator = DecisionTreeRegressor(random_state=0)


seletor = SelectFromModel(estimator, threshold=0, max_features=6)
seletor = seletor.fit(X,y)


seletor.get_feature_names_out()





print()
