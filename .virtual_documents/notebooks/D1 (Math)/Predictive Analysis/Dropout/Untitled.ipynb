import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

d1 = pd.read_csv("d1_trabalhadoDrop.csv", sep=",")


X = d1.drop(['dropout'], axis=1)
y = d1['dropout']


from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif, mutual_info_classif

f_classif = SelectKBest(score_func=f_classif, k=7)
# k = 4 (é o número de variáveis selecionadas)


fit = f_classif.fit(X, y)


features = fit.transform(X)
print(features)


cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = d1.drop('dropout', axis=1)
y = d1['dropout']

test = SelectKBest(chi2, k=7)
fit = test.fit(X,y)

fit.get_support(indices=True)

features = fit.transform(X)

cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=2000)

from sklearn.feature_selection import RFE

model = LogisticRegression(solver='saga', max_iter=5000)
rfe = RFE(model, n_features_to_select=7, step=2)

fit = rfe.fit(X,y)


print("Número de features: {}".format(fit.n_features_))


cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.ensemble import RandomForestRegressor



