import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

d1 = pd.read_csv("d1_trabalhado.csv", sep=",")


X = d1.drop(['G3'], axis=1)
y = d1['G3']








from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

f_regression = SelectKBest(score_func=f_regression, k=7)
# k = 4 (é o número de variáveis selecionadas)


fit = f_regression.fit(X, y)


features = fit.transform(X)


print(features)


cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE


model = LogisticRegression(solver='saga', max_iter=5000)
rfe = RFE(model, n_features_to_select=6, step=2)


fit = rfe.fit(X,y)


print("Número de features: {}".format(fit.n_features_))


cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.ensemble import RandomForestRegressor


model = RandomForestRegressor(n_estimators=10)
model.fit(X,y)


print(model.feature_importances_)


feature_importances = pd.DataFrame(model.feature_importances_,
                                   index = X.columns,
                                   columns = ['importance']).sort_values('importance',
                                   ascending=False)


feature_importances


plt.figure(figsize=(20, 8))
feature_importances.plot(kind='bar')





from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.15)
selector.fit(X)

features_selecionadas = X.columns[selector.get_support()]
features_removidas = X.columns[~selector.get_support()]

print("Features selecionadas:", features_selecionadas)
print("Features removidas:", features_removidas)


# criando gráfico
variances = X.var()

data = {
    'Feature': X.columns,
    'Variance': variances
}
df_variance = pd.DataFrame(data)

df_variance['Status'] = df_variance['Feature'].apply(lambda x: 'Selecionada' if x in features_selecionadas else 'Removida')

# Plotar
plt.figure(figsize=(16, 8))
bars = plt.bar(df_variance['Feature'], df_variance['Variance'], color=df_variance['Status'].map({'Selecionada': 'blue', 'Removida': 'red'}))
plt.xlabel('Feature')
plt.ylabel('Variância')
plt.title('Variância das Features Selecionadas e Removidas')

plt.axhline(y=0.15, color='gray', linestyle='--', label='Threshold')
plt.legend(['Threshold', 'Selecionada', 'Removida'])
plt.xticks(rotation=90) 

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()





from sklearn.tree import DecisionTreeRegressor
clf = DecisionTreeRegressor(random_state=0)


clf = clf.fit(X,y)


clf.feature_importances_


feature_names = X.columns
importances = clf.feature_importances_

feature_importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

feature_importances = feature_importances.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(16, 8))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.gca().invert_yaxis()
plt.show()





from sklearn.feature_selection import SelectFromModel


estimator = DecisionTreeRegressor(random_state=0)


seletor = SelectFromModel(estimator, threshold=0, max_features=6)
seletor = seletor.fit(X,y)


seletor.get_feature_names_out()





# Identificando pares de features com alta correlação
corr_matrix = X.corr().abs()

high_corr_pairs = []

for i in range(len(corr_matrix.columns)):
    for j in range(i + 1, len(corr_matrix.columns)):
        if corr_matrix.iloc[i, j] > 0.8: 
            high_corr_pairs.append({
                'Feature1': corr_matrix.columns[i],
                'Feature2': corr_matrix.columns[j],
                'Correlation': corr_matrix.iloc[i, j]
            })

high_corr_pairs_df = pd.DataFrame(high_corr_pairs)
print(high_corr_pairs_df)





# Calculando o VIF para cada feature
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

X_with_const = sm.add_constant(X)

vif_data = pd.DataFrame()
vif_data['Feature'] = X_with_const.columns
vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]

print(vif_data)





X = d1.drop(['G3', 'G2', 'G1'], axis=1)
y = d1['G3']





from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

f_regression = SelectKBest(score_func=f_regression, k=6)
fit = f_regression.fit(X, y)
features = fit.transform(X)
print(features)

cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE

model = LogisticRegression(solver='saga', max_iter=5000)
rfe = RFE(model, n_features_to_select=6, step=2)

fit = rfe.fit(X,y)

print("Número de features: {}".format(fit.n_features_))

cols = fit.get_support(indices=True)
d1.iloc[:, cols]





from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=10)
model.fit(X,y)
print(model.feature_importances_)
feature_importances = pd.DataFrame(model.feature_importances_,
                                   index = X.columns,
                                   columns = ['importance']).sort_values('importance',
                                   ascending=False)
feature_importances


plt.figure(figsize=(20, 8))
feature_importances.plot(kind='bar')





from sklearn.tree import DecisionTreeRegressor
clf = DecisionTreeRegressor(random_state=0)

clf = clf.fit(X,y)

clf.feature_importances_

feature_names = X.columns
importances = clf.feature_importances_

feature_importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

feature_importances = feature_importances.sort_values(by='Importance', ascending=False)
plt.figure(figsize=(16, 8))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.gca().invert_yaxis()
plt.show()





from sklearn.feature_selection import SelectFromModel
estimator = DecisionTreeRegressor(random_state=0)
seletor = SelectFromModel(estimator, threshold=0, max_features=6)
seletor = seletor.fit(X,y)
seletor.get_feature_names_out()



