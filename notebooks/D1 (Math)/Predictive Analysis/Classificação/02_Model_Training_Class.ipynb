{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8331052c-55a9-47ec-9336-2863ce28c1fa",
   "metadata": {},
   "source": [
    "Além do KNN, existem várias outras técnicas e modelos que você pode utilizar para realizar tarefas de classificação, dependendo das características dos seus dados e dos objetivos do seu modelo. Aqui estão algumas opções populares:\n",
    "\n",
    "### 1. **Árvore de Decisão (Decision Tree)**\n",
    "   - **Vantagens:** Fácil de interpretar, não requer normalização das features, bom para capturar interações entre as features.\n",
    "   - **Desvantagens:** Pode facilmente sofrer de overfitting se não for podada ou regularizada.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "     clf = DecisionTreeClassifier(random_state=42)\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### 2. **Random Forest**\n",
    "   - **Vantagens:** Reduz o overfitting por meio de um conjunto de árvores de decisão, muito robusto e eficaz em muitos cenários.\n",
    "   - **Desvantagens:** Menos interpretável do que uma única árvore de decisão.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "     clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### 3. **Gradient Boosting Machines (GBM)**\n",
    "   - **Vantagens:** Muito poderoso, pode fornecer excelente performance em uma ampla gama de problemas.\n",
    "   - **Desvantagens:** Mais lento para treinar, especialmente em comparação com Random Forest.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "     clf = GradientBoostingClassifier(random_state=42)\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### 4. **Logistic Regression**\n",
    "   - **Vantagens:** Simples, interpretável, funciona bem com dados lineares, fornece probabilidades de predição.\n",
    "   - **Desvantagens:** Desempenho limitado em problemas não lineares.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "     clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### 5. **Support Vector Machine (SVM)**\n",
    "   - **Vantagens:** Eficaz em espaços de alta dimensionalidade, robusto a outliers, pode ser usado com diferentes kernels para capturar relações não lineares.\n",
    "   - **Desvantagens:** Pode ser lento em conjuntos de dados muito grandes, sensível à escolha de parâmetros.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from sklearn.svm import SVC\n",
    "\n",
    "     clf = SVC(kernel='rbf', random_state=42)\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### 6. **Naive Bayes**\n",
    "   - **Vantagens:** Muito rápido e eficiente, bom para problemas de texto e dados categóricos, não requer muita amostra para funcionar bem.\n",
    "   - **Desvantagens:** Assume independência entre as features, o que pode não ser realista em muitos casos.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "     clf = GaussianNB()\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### 7. **Neural Networks (MLP Classifier)**\n",
    "   - **Vantagens:** Muito flexível e poderoso, capaz de modelar relações complexas, especialmente com muitas features.\n",
    "   - **Desvantagens:** Requer mais dados e poder de processamento, tuning de hiperparâmetros pode ser desafiador.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "     clf = MLPClassifier(random_state=42, max_iter=1000)\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### 8. **XGBoost**\n",
    "   - **Vantagens:** Um dos melhores algoritmos de boosting, muito eficiente em termos de tempo de execução, excelente performance em muitos problemas.\n",
    "   - **Desvantagens:** Requer tuning de hiperparâmetros para obter o melhor desempenho.\n",
    "   - **Implementação:**\n",
    "     ```python\n",
    "     from xgboost import XGBClassifier\n",
    "\n",
    "     clf = XGBClassifier(random_state=42)\n",
    "     clf.fit(X_train, y_train)\n",
    "     y_pred = clf.predict(X_val)\n",
    "     ```\n",
    "\n",
    "### Comparação e Seleção do Modelo:\n",
    "- **Avaliação de Modelos:** Use técnicas como validação cruzada, análise de curva ROC-AUC, F1-score e outras métricas para comparar a performance dos diferentes modelos.\n",
    "- **Ensembles:** Considere combinar vários desses modelos para criar um ensemble que possa melhorar a performance geral (ex: Voting Classifier).\n",
    "\n",
    "### Conclusão:\n",
    "A escolha do modelo ideal dependerá das características dos dados e do problema em questão. Pode valer a pena experimentar vários desses métodos, comparar suas performances, e selecionar o que oferece o melhor equilíbrio entre acurácia, interpretabilidade e eficiência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "141de46d-4e6c-45d7-9fce-16a272fddc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d1_trabalhadoC = pd.read_csv(\"d1_trabalhadoC.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ff956c7-1bf7-4b7e-bae9-ce9d107baa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(d1_trabalhadoC)\n",
    "d1_trabalhadoC = pd.DataFrame(scaler.transform(d1_trabalhadoC), columns = d1_trabalhadoC.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1b3ffb99-ad81-4949-843f-2b4de574a825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['failures', 'Medu', 'internet', 'absences', 'studytime']\n",
      "Target: G3_excelente\n"
     ]
    }
   ],
   "source": [
    "target = 'failures'\n",
    "features = ['failures', 'Medu', 'internet','absences']\n",
    "\n",
    "print('Features:', features)\n",
    "print('Target:', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2881887f-c377-40e8-b2ee-a73df9cf4c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 5)\n",
      "(79, 5)\n",
      "(316,)\n",
      "(79,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(d1_trabalhadoC[features], d1_trabalhadoC[target],\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533ae48-6244-48fa-a2cd-084c36426746",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "66d13fc2-1242-4367-88c5-69667ef96d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=7)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando nosso classificador\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=7)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e0f851ae-fc4d-4042-b6ea-ce7406db6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bianca\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "bda508e1-e25d-4bab-9fac-72e50ff452f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.7848101265822784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print('Acurácia:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38996185-4a6c-4e72-be83-1c24fc6dcf83",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7e8385f-03fb-412f-aec0-9b7c67f043e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
       "       'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid',\n",
       "       'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel',\n",
       "       'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences',\n",
       "       'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services',\n",
       "       'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other',\n",
       "       'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home',\n",
       "       'reason_other', 'reason_reputation', 'guardian_father',\n",
       "       'guardian_mother', 'guardian_other', 'G1_insuficiente', 'G1_regular',\n",
       "       'G1_excelente', 'G2_insuficiente', 'G2_regular', 'G2_excelente',\n",
       "       'G3_insuficiente', 'G3_regular', 'G3_excelente'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1_trabalhadoC.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dcc2735-2615-4c55-8676-80338d3969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "\n",
    "target = 'G3_excelente'\n",
    "features = ['failures', 'Medu', 'absences', 'famsup', 'internet', 'studytime']\n",
    "\n",
    "X = d1_trabalhadoC[features]\n",
    "y = d1_trabalhadoC[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c93905f2-3e5c-4f4d-80b9-e3d6d26dd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70b8aac4-110b-40d0-8898-c2f2041983b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.92      0.83        59\n",
      "           1       0.38      0.15      0.21        20\n",
      "\n",
      "    accuracy                           0.72        79\n",
      "   macro avg       0.57      0.53      0.52        79\n",
      "weighted avg       0.66      0.72      0.67        79\n",
      "\n",
      "Confusion Matrix:\n",
      "[[54  5]\n",
      " [17  3]]\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
